{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6frhC9jxkeL"
   },
   "source": [
    "# 1. Sourcing PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mW--yPyxxSF"
   },
   "source": [
    "PDF link: https://www.ashtoncentralmosque.com/app/uploads/2014/07/the-quran-with-annotated-interpretation-in-modern-english-ali-unal.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99qrIWnxxd5p",
    "outputId": "3d1b9bbb-85c9-4e30-9b42-5778aa22676d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "pdf_path = \"the-quran-with-annotated-interpretation-in-modern-english-ali-unal.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"File does not exists, downloading...\")\n",
    "\n",
    "    url = \"https://www.ashtoncentralmosque.com/app/uploads/2014/07/the-quran-with-annotated-interpretation-in-modern-english-ali-unal.pdf\"\n",
    "\n",
    "    filename = pdf_path\n",
    "\n",
    "    response = requests.get(url) #gets file in bytes\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      with open(pdf_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "      print(\"File downloaded successfully.\")\n",
    "    else:\n",
    "      print(f\"Failed to download the file {response.status_code}\")\n",
    "else:\n",
    "  print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROtycy8YTO0G"
   },
   "source": [
    "#2. Extracting text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9skhcK10x48R",
    "outputId": "da229b0d-21be-4443-cb41-357e26ead915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.8)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "\n",
    "import re\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "pdf_path = \"/content/the-quran-with-annotated-interpretation-in-modern-english-ali-unal.pdf\"\n",
    "\n",
    "def remove_inline_numbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove citation-like numbers that come right after a period or ')'.\n",
    "    Example: 'repentance).22 God' -> 'repentance). God'\n",
    "    \"\"\"\n",
    "    pattern = r'([.)])\\s*\\d+'\n",
    "    return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "chunks = []  # list of dicts: {\"page_number\": int, \"chunks_text\": list[str]}\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        page_number = page.page_number\n",
    "        raw_text = page.extract_text() or \"\"\n",
    "\n",
    "        cleaned_text = remove_inline_numbers(raw_text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "        page_chunks = []\n",
    "        current_chunk = []\n",
    "\n",
    "        for i, sent in enumerate(sentences, start=1):\n",
    "            current_chunk.append(sent)\n",
    "            if i % 10 == 0:\n",
    "                page_chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "        if current_chunk:\n",
    "            page_chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        chunks.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"chunks_text\": page_chunks\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks)\n",
    "\n",
    "chunks_df = chunks_df.explode(\"chunks_text\", ignore_index=True)\n",
    "\n",
    "chunks_df[\"chunk_id_in_page\"] = (\n",
    "    chunks_df.groupby(\"page_number\").cumcount() + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1yWQ_AJ_bnQ",
    "outputId": "5a6f54b3-3a5b-4406-a747-b6e3c39c8d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      page_number                                        chunks_text  \\\n",
      "2935         1317  1317 Glossary of Terms\\n(ash-)Sharī‘at at-Takw...   \n",
      "2936         1317  If this Prophetic description is figurative, i...   \n",
      "2937         1318  Glossary of Terms 1318\\nSubhānallāh: All-Glori...   \n",
      "2938         1318  It has two aspects, one for the life of all cr...   \n",
      "2939         1319  1319 Glossary of Terms\\n(as-)Sūrah: An indepen...   \n",
      "2940         1320  Glossary of Terms 1320\\ncan be accomplished th...   \n",
      "2941         1321  1321 Glossary of Terms\\nservice\\n[J14] at-)Tas...   \n",
      "2942         1322  Glossary of Terms 1322\\nable meanings. (at-)Ta...   \n",
      "2943         1323  1323 Glossary of Terms\\nand at-tawāf, and as-s...   \n",
      "2944         1324  Glossary of Terms 1324\\n(al-)Yahūd: The Jews. ...   \n",
      "2945         1324  The Qur’ān uses the word “day” not only in the...   \n",
      "2946         1325  1325 Glossary of Terms\\nback to me,” meaning h...   \n",
      "2947         1325  Having a very wide area of usage, in the termi...   \n",
      "2948         1326  The End\\nTo appreciate the writer’s work pleas...   \n",
      "2949         1327  For more information:\\nwww.mquran.org\\nwww.the...   \n",
      "\n",
      "      chunk_id_in_page  \n",
      "2935                 1  \n",
      "2936                 2  \n",
      "2937                 1  \n",
      "2938                 2  \n",
      "2939                 1  \n",
      "2940                 1  \n",
      "2941                 1  \n",
      "2942                 1  \n",
      "2943                 1  \n",
      "2944                 1  \n",
      "2945                 2  \n",
      "2946                 1  \n",
      "2947                 2  \n",
      "2948                 1  \n",
      "2949                 1  \n"
     ]
    }
   ],
   "source": [
    "print(chunks_df.tail(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D01vuIeDOFOC"
   },
   "source": [
    "- Embeddings model: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "- Base transformer: microsoft/mpnet-base (MPNet encoder)\n",
    "- Maps sentences and paragraphs to a 768‑dimensional dense vector space\n",
    "- Model size 0.1B params\n",
    "- Trained on 3 different datasets\n",
    "- dataset 1 link: https://huggingface.co/datasets/mandarjoshi/trivia_qa\n",
    "- dataset 2 link: https://huggingface.co/datasets/stanfordnlp/snli\n",
    "- dataset 3 link: https://huggingface.co/datasets/google-research-datasets/natural_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E64b_9-DDHpP"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59ap5Rcx4RsN",
    "outputId": "eea4ace5-8b03-4ad4-a774-a0b414718381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 10:36:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0             26W /   70W |     574MiB /  15360MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDi3JEo4OOgU",
    "outputId": "bf2e0119-92f8-4962-81b1-fd8a047680e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page_number                                        chunks_text  \\\n",
      "0            1                                                NaN   \n",
      "1            2  THE QUR’AN\\nwith\\nAnnotated Interpretation in ...   \n",
      "2            3                www.mquran.org\\nwww.theholybook.org   \n",
      "3            4  Contents\\nForeword ..............................   \n",
      "4            4  Yunus (Jonah) ...................................   \n",
      "\n",
      "   chunk_id_in_page                                         embeddings  \n",
      "0                 1  [-0.02318713814020157, 0.05149746313691139, -0...  \n",
      "1                 1  [0.027168413624167442, 0.0320991687476635, 0.0...  \n",
      "2                 1  [0.017434922978281975, 0.06314095854759216, -0...  \n",
      "3                 1  [0.01651417650282383, -0.013219342567026615, -...  \n",
      "4                 2  [0.022808723151683807, 0.05383909493684769, -0...  \n",
      "[-0.02318713814020157, 0.05149746313691139, -0.002392231021076441, -0.008844197727739811, -0.01957680843770504, 0.024297600612044334, 0.023422790691256523, -0.0016432525590062141, -0.04026688635349274, 0.01920575648546219]\n"
     ]
    }
   ],
   "source": [
    "# encode all chunk texts as a list\n",
    "chunk_texts = chunks_df[\"chunks_text\"].tolist()\n",
    "embeddings = embedding_model.encode(chunk_texts, convert_to_numpy=True)  # or convert_to_tensor=True\n",
    "\n",
    "# add as new column (store as list so DataFrame can handle it)\n",
    "chunks_df[\"embeddings\"] = embeddings.tolist()\n",
    "\n",
    "print(chunks_df.head())\n",
    "print(chunks_df.iloc[0][\"embeddings\"][:10])  # first 10 dims of first chunk vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlRgIrLGUlq1"
   },
   "source": [
    "# 3. FAISS vector database\n",
    "\n",
    "- Github link: https://github.com/facebookresearch/faiss\n",
    "- FAISS documentation: https://faiss.ai\n",
    "- For deeper understanding of FAISS: https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rWT5GsLQMcC",
    "outputId": "a065e7c9-8c50-4df7-d059-b7e5d9818a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "(2950, 768)\n",
      "Is trained: True\n",
      "Index size (ntotal):\n",
      "2950\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "emb_matrix = np.vstack(chunks_df[\"embeddings\"].values).astype(\"float32\")\n",
    "print(emb_matrix.shape)  # (num_chunks, 768)\n",
    "\n",
    "faiss.normalize_L2(emb_matrix)\n",
    "\n",
    "d = emb_matrix.shape[1]  # embedding dimension (768 for all-mpnet-base-v2)\n",
    "\n",
    "index = faiss.IndexFlatIP(d)  # exact search, inner product\n",
    "print(\"Is trained:\", index.is_trained)\n",
    "\n",
    "index.add(emb_matrix)\n",
    "print(\"Index size (ntotal):\")\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouIT24BrXoON",
    "outputId": "78e3d4ee-ff5d-4bbe-c6f3-1471b643178c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 10:37:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P0             29W /   70W |    1540MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCrO6uV_hOKm"
   },
   "source": [
    "~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVwcumutmcd-",
    "outputId": "44b0a87e-e7f5-4c59-967f-586d1a4a58ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "HF_TOKEN = \"hf_huggingface_token\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzGFyXptusa8"
   },
   "source": [
    "# 4. Model (Google Gemma 2b-it)\n",
    "- Model Link: https://huggingface.co/google/gemma-2-2b-it\n",
    "- Base model \"google/gemma-2-2b\" link: https://huggingface.co/google/gemma-2-2b\n",
    "- Model size 3B params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567,
     "referenced_widgets": [
      "e6954df3c706468b83067c93385a197e",
      "2aad9d5e337d406cae0631f24f77ef1c",
      "eac513b369ff475a91d1d32049bee3eb",
      "61919f8791ca40c8903a206c42f0de01",
      "201c5cba3dd448c6867873295a573ad4",
      "ca1cf3e37f8148fbacecec7eb3ee06f4",
      "37fd6ed190244c06998cb3db62137c83",
      "12f2e13933b745369cdf1ede873ead90",
      "9438ed6da9034984b877fab48bd4f913",
      "21c3a60aedf74724a723cd3cf704cb21",
      "dc77b73f71ad4933be6200b737ccea39",
      "f392d068efd842b38390bd134278dd09",
      "9fd305bfab7244758a1878e86fe1787e",
      "29119acd21d04d3da3198f2a6b1307e8",
      "6a7a6d2678d745a0b05856db787ace12",
      "2c2812f4a4bc417bb8f815bf0edb449d",
      "81eafb0728574f3890bb1f693b359f4b",
      "b3ba952aa4bb451083ca3278569a31c0",
      "f0835c27ea7a48578ca9fb1d7269d891",
      "8c26e9e8e6f24cf2b250ad1285adcc73",
      "70212d803083402a9dca5c00b9e43218",
      "99aa87bab1fe45219d16191c966869ee"
     ]
    },
    "id": "g666xc1OhM-c",
    "outputId": "9f7e7cfb-ecfa-4aa8-9093-850d73eba67c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6954df3c706468b83067c93385a197e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392d068efd842b38390bd134278dd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "HF_TOKEN = \"hf_huggingface_token\"\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HF_TOKEN,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",          # send weights directly to GPU\n",
    "    low_cpu_mem_usage=True,     # avoid big CPU fp32 copy\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzyYCKSZhzBp",
    "outputId": "a5bd21f8-020d-49b4-ca78-fc57972b09c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 10:38:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0             29W /   70W |    4696MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XLLWyJvRnis8"
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k(query, k=8):\n",
    "    q_emb = embedding_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    contexts = []\n",
    "    for score, i in zip(scores, idxs):\n",
    "        row = chunks_df.iloc[i]\n",
    "        contexts.append(row[\"chunks_text\"])\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IPjgebg5tLmx"
   },
   "outputs": [],
   "source": [
    "def build_rag_prompt(question, contexts):\n",
    "    context_block = \"\\n\\n\".join(contexts)\n",
    "    return (\n",
    "        \"You are an assistant answering questions about the Qur'an.\\n\"\n",
    "        \"Use ONLY the context below. If the answer is not there, say you don't know.\\n\\n\"\n",
    "        f\"Context:\\n{context_block}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer clearly in English.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7Yar11hwtPry"
   },
   "outputs": [],
   "source": [
    "def answer_question(question, k=8, max_new_tokens=256):\n",
    "    contexts = retrieve_top_k(question, k=k)\n",
    "    user_prompt = build_rag_prompt(question, contexts)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    generated = outputs[0][input_ids.shape[-1]:]\n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eEdwkHXtUsN",
    "outputId": "a3ae887e-b1d5-4c60-97d9-4b219eff1911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Quran describes the Day of Judgment as a day of ultimate reckoning and consequence. \n",
      "\n",
      "Here are some key points from the verses you provided:\n",
      "\n",
      "* **Silence and No Excuses:**  On Judgment Day, people will be unable to speak or offer excuses (verse 35).\n",
      "* **Diverse Manifestations:** The Day of Judgment will not be a single, uniform event.  People will experience different types of punishment and reward, including Hell and Paradise (verses 36, 45, 46, 47). \n",
      "* **Unseen Judgment:** The Quran emphasizes that the Day of Judgment is not a spectacle but a time for individual accountability.  People will face their deeds and be judged by God (verses 36, 45, 46, 47).\n",
      "* **Universal Gathering:** The Day of Judgment will see all humanity, including the jinn, gathered together (verse 36, 47).\n",
      "* **The Trumpet and the Last Hour:** The Day of Judgment will be heralded by the sound of the Trumpet, and the Last Hour will be a time of intense struggle and punishment for the disbelievers (verse 753).\n",
      "* **Rewards and Punishments\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the quran say about the day of judgement\"\n",
    "print(answer_question(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2_ptoSj61bW",
    "outputId": "db4429cf-60d5-43cf-fe0c-fd934f434937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provides several options for those who cannot fast in Ramadan. \n",
      "\n",
      "* **They can feed a person in need.** This is a form of redemption (penance) for those who are unable to fast. \n",
      "* **They can make up for the missed days.** The text states they must make up the number of days they missed during Ramadan.\n",
      "* **They can continue with supplicatory prayer.** This refers to prayers during the month of Ramadan.\n",
      "* **They must not be on a journey or be ill.** These are exceptions to the general rule.\n",
      "\n",
      "\n",
      "The context emphasizes that it's important to make up for missed days and that fasting is beneficial.\n"
     ]
    }
   ],
   "source": [
    "question = \"What if someone donot fast in the month of ramadan\"\n",
    "print(answer_question(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I1SNQdQ5DL8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
